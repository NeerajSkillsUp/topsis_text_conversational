# ðŸ“Š Conversational AI Evaluation
An end-to-end framework for evaluating and ranking pre-trained conversational text models using the TOPSIS (Technique for Order of Preference by Similarity to Ideal Solution) algorithm.

This project provides a robust methodology to compare AI models (like GPT-2, DialoGPT, and OPT) based on a multi-criteria decision-making process. It balances linguistic quality against operational efficiency.

# ðŸ›  Methodology
The evaluation follows a structured multi-criteria decision analysis. Each model is graded on three distinct pillars: Accuracy (BLEU), Fluency (ROUGE), and Efficiency (Latency).

# ðŸ“ˆ Final Results
Below is the performance table generated by the framework:
<img width="1135" height="294" alt="image" src="https://github.com/user-attachments/assets/ddb889ff-2885-4892-9a26-ded2f742bc67" />

Below are the visuals:

<img width="453" height="390" alt="image" src="https://github.com/user-attachments/assets/bde572f1-85d6-4305-b8d9-c2783296295b" />

<img width="698" height="347" alt="image" src="https://github.com/user-attachments/assets/4518010f-db48-41d7-963e-e0c9ea7cc2eb" />

